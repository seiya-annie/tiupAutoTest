import os
import subprocess
import random
import time
import threading
from uuid import uuid4
from flask import Flask, render_template, request, jsonify, session, send_from_directory
from packaging.version import Version
import mysql.connector
import sys
import stat
import ast

TIDB_GO_VERSION_MAP = {
    "4.0": "1.13.15",
    "5.0": "1.13.15",
    "5.1": "1.16.15",
    "5.2": "1.16.15",
    "5.3": "1.16.15",
    "5.4": "1.16.15",
    "6.0": "1.18.10",
    "6.1": "1.18.10",
    "6.2": "1.18.10",
    "6.3": "1.19.13",
    "6.4": "1.19.13",
    "6.5": "1.19.13",
    "6.6": "1.19.13",
    "7.0": "1.20.14",
    "7.1": "1.20.14",
    "7.2": "1.20.14",
    "7.3": "1.20.14",
    "7.4": "1.21.13",
    "7.5": "1.21.13",
    "7.6": "1.21.13",
    "8.0": "1.21.13",
    "8.1": "1.21.13",
    "8.2": "1.21.13",
    "8.3": "1.21.13",
    "8.4": "1.23.6",
    "8.5": "1.25.1",
    # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦ç»§ç»­æ·»åŠ æ–°çš„ç‰ˆæœ¬æ˜ å°„
}
DEFAULT_GO_VERSION = "1.25.1"

COMPONENT_COUNTS = {
    'tidb': 1,
    'tikv': 1,
    'pd': 1,
    'tiflash': 0
}

# --- é…ç½® ---
app = Flask(__name__)
# ç”¨äº session åŠ å¯†ï¼Œè¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ›¿æ¢ä¸ºæ›´å¤æ‚çš„å¯†é’¥
app.secret_key = 'a_very_secret_key_for_tidb_tester_tiup'

# ç”¨äºå­˜å‚¨åå°ä»»åŠ¡çš„çŠ¶æ€å’Œç»“æœ
# tasks å­—å…¸ç°åœ¨ä¹Ÿå­˜å‚¨ Popen è¿›ç¨‹å¯¹è±¡ï¼Œä»¥ä¾¿åç»­æ¸…ç†
tasks = {}
# TiDB ç¼–è¯‘åçš„äºŒè¿›åˆ¶æ–‡ä»¶ç›¸å¯¹è·¯å¾„
TIDB_BINARY_PATH = "bin/tidb-server"
# ç¼–è¯‘å‘½ä»¤
COMPILE_COMMAND = "make"
TIDB_REPO_PATH = '/Users/lt/git/tidb'


# --- commit äºŒåˆ†æŸ¥æ‰¾å‡½æ•° --
def run_command(command, work_dir=".", shell=False, check=True, print_output=False):
    """ä¸€ä¸ªé€šç”¨çš„å‘½ä»¤æ‰§è¡Œå‡½æ•°ï¼Œå®æ—¶æ‰“å°è¾“å‡º"""
    print(f"ğŸš€ åœ¨ '{work_dir}' ä¸­æ‰§è¡Œ: {' '.join(command) if isinstance(command, list) else command}")

    if isinstance(command, list):
        # å°†åˆ—è¡¨å‘½ä»¤å®‰å…¨åœ°æ‹¼æ¥æˆå­—ç¬¦ä¸²
        command_str = ' '.join(f"'{arg}'" if ' ' in arg else arg for arg in command)
    else:
        command_str = command

    asdf_script_path = os.path.expanduser("~/.asdf/asdf.sh")
    if not os.path.exists(asdf_script_path):
        print(f"âŒ é”™è¯¯: asdf ç¯å¢ƒè„šæœ¬æœªåœ¨ '{asdf_script_path}' æ‰¾åˆ°ã€‚")
        sys.exit(1)
    # ä½¿ç”¨ bash -c '...' æ¥ç¡®ä¿åœ¨ä¸€ä¸ª shell ä¸­å…ˆ source å†æ‰§è¡Œå‘½ä»¤
    final_command = f". {asdf_script_path} && {command_str}"
    final_command_list = ["/bin/bash", "-li", "-c", final_command]

    if print_output:
        print(f"ğŸš€ (In Bash with ASDF Env) åœ¨ '{work_dir}' ä¸­æ‰§è¡Œ: {final_command}")

    try:
        process = subprocess.Popen(
            final_command_list,
            cwd=work_dir,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            shell=shell,
            preexec_fn=os.setsid if sys.platform != "win32" else None
        )

        output_lines, full_output = [], ""
        if print_output:
            for line in iter(process.stdout.readline, ''):
                sys.stdout.write(line)
                output_lines.append(line)
            full_output = "".join(output_lines)

        process.wait()

        if not print_output:
            full_output = process.stdout.read()

        if check and process.returncode != 0:
            print("compile fail:", process.stderr)
            raise subprocess.CalledProcessError(process.returncode, command)

        return full_output
    except FileNotFoundError:
        command_name = command[0] if isinstance(command, list) else command.split()[0]
        print(f"âŒ å‘½ä»¤æœªæ‰¾åˆ°: {command[0]}. è¯·ç¡®ä¿å®ƒå·²å®‰è£…å¹¶åœ¨æ‚¨çš„ PATH ä¸­ã€‚")
        sys.exit(1)


def get_commit_list(start_tag, end_tag, task_id):
    """è·å–ä¸¤ä¸ª tag ä¹‹é—´çš„ commit SHA åˆ—è¡¨ï¼ŒæŒ‰æ—¶é—´æ­£åºæ’åˆ—"""
    tasks[task_id]['log'].append(f"\nâ„¹ï¸ å‡†å¤‡åˆ‡æ¢åˆ°ä¸ tag '{end_tag}' ç›¸å…³çš„ release åˆ†æ”¯...")
    try:
        # ä» 'v8.5.0' -> '8.5.0' -> ['8', '5', '0'] -> '8.5'
        version_parts = end_tag.lstrip('v').split('.')
        branch_version = f"{version_parts[0]}.{version_parts[1]}"
        branch_name = f"release-{branch_version}"

        # ä½¿ç”¨ -f å¼ºåˆ¶åˆ‡æ¢ï¼Œå¿½ç•¥æœ¬åœ°æœªæäº¤çš„ä¿®æ”¹
        run_command(["git", "checkout", "-f", branch_name], work_dir=TIDB_REPO_PATH)
        tasks[task_id]['log'].append(f"âœ… æˆåŠŸåˆ‡æ¢åˆ°åˆ†æ”¯: {branch_name}")

    except IndexError:
        tasks[task_id]['log'].append(f"âš ï¸ è­¦å‘Š: æ— æ³•ä» tag '{end_tag}' æ¨æ–­å‡ºæ ‡å‡†çš„ release åˆ†æ”¯åã€‚è·³è¿‡ checkoutã€‚")
        return
    except subprocess.CalledProcessError:
        tasks[task_id]['log'].append(f"âš ï¸ è­¦å‘Š: åˆ‡æ¢åˆ°åˆ†æ”¯ '{branch_name}' å¤±è´¥ã€‚è¯¥åˆ†æ”¯å¯èƒ½åœ¨æœ¬åœ°ä¸å­˜åœ¨ã€‚")
        tasks[task_id]['log'].append("ç»§ç»­å°è¯•ç›´æ¥ä½¿ç”¨ tag è¿›è¡Œ commit æŸ¥æ‰¾...")
        return

    try:
        command = ["git", "rev-list", "--reverse", f"{start_tag}..{end_tag}"]
        output = run_command(command, TIDB_REPO_PATH)
        tasks[task_id]['log'].append(f"\nğŸ” è·å– {start_tag}..{end_tag} ä¹‹é—´çš„ commit åˆ—è¡¨...")
        # --reverse å‚æ•°è®© commit ä»æ—§åˆ°æ–°æ’åˆ—ï¼Œç¬¦åˆäºŒåˆ†æŸ¥æ‰¾çš„é€»è¾‘é¡ºåº
        commits = output.strip().split('\n')
        tasks[task_id]['log'].append(f"âœ… æ‰¾åˆ° {len(commits)} ä¸ª commitsã€‚")
        return [c for c in commits if c]
    except Exception as e:
        tasks[task_id]['log'].append(f"âŒ get commits list fail")
        # åœ¨äºŒåˆ†æŸ¥æ‰¾ä¸­ï¼Œç¼–è¯‘å¤±è´¥é€šå¸¸è¢«è§†ä¸º "bad" commit
        return None


def compile_at_commit(commit_sha, task_id, version):
    """Checkout åˆ°æŒ‡å®š commit å¹¶è¿›è¡Œç¼–è¯‘"""
    tasks[task_id]['log'].append(f"\nğŸ”§ åˆ‡æ¢åˆ° commit: {commit_sha[:8]} å¹¶å¼€å§‹ç¼–è¯‘...")
    try:
        if version == 'master':
            go_version = DEFAULT_GO_VERSION
        else:
            version_key = ".".join(version.lstrip('v').split('.')[:2])
            go_version = TIDB_GO_VERSION_MAP.get(version_key, DEFAULT_GO_VERSION)

            if version_key not in TIDB_GO_VERSION_MAP:
                print(f"âš ï¸ è­¦å‘Š: åœ¨ç‰ˆæœ¬æ˜ å°„ä¸­æœªæ‰¾åˆ° '{version_key}'ã€‚å°†ä½¿ç”¨é»˜è®¤ Go ç‰ˆæœ¬: {DEFAULT_GO_VERSION}")

        run_command(["git", "checkout", commit_sha], work_dir=TIDB_REPO_PATH)

        print(f"âš™ï¸ æ­£åœ¨ä¸º TiDB ç‰ˆæœ¬ '{version}' è®¾ç½® Go ç‰ˆæœ¬ä¸º: {go_version}...")
        run_command(["asdf", "local", "go", go_version], work_dir=TIDB_REPO_PATH)

        # éªŒè¯ Go ç‰ˆæœ¬æ˜¯å¦åˆ‡æ¢æˆåŠŸ
        print("Verifying Go version...")
        run_command(["go", "version"], work_dir=TIDB_REPO_PATH)
    except Exception as e:
        print(f"âŒ è®¾ç½® Go ç‰ˆæœ¬æ—¶å‡ºé”™: {e}ã€‚å°†ä½¿ç”¨ç¯å¢ƒä¸­å·²æœ‰çš„ Go ç‰ˆæœ¬ç»§ç»­å°è¯•ã€‚")
        return

    try:
        # ç¼–è¯‘ TiDB server
        run_command(COMPILE_COMMAND.split(), work_dir=TIDB_REPO_PATH)

        binary_full_path = os.path.join(TIDB_REPO_PATH, TIDB_BINARY_PATH)
        if not os.path.exists(binary_full_path):
            raise FileNotFoundError(f"ç¼–è¯‘äº§ç‰© {binary_full_path} æœªæ‰¾åˆ°ï¼")

        tasks[task_id]['log'].append(f"âœ… ç¼–è¯‘æˆåŠŸ: {binary_full_path}")
        return binary_full_path
    except subprocess.CalledProcessError as e:
        tasks[task_id]['log'].append(f"âŒ åœ¨ commit {commit_sha[:8]} ç¼–è¯‘å¤±è´¥ï¼")
        # åœ¨äºŒåˆ†æŸ¥æ‰¾ä¸­ï¼Œç¼–è¯‘å¤±è´¥é€šå¸¸è¢«è§†ä¸º "bad" commit
        return None

# --- è¾…åŠ©å‡½æ•° ---


def get_tidb_versions():
    """é€šè¿‡ tiup list tidb è·å–å¯ç”¨çš„ TiDB ç‰ˆæœ¬åˆ—è¡¨"""
    try:
        subprocess.run(["tiup", "update", "--self"], check=True, capture_output=True, text=True, timeout=120)
        result = subprocess.run(
            ["tiup", "list", "tidb"],
            check=True,
            capture_output=True,
            text=True,
            timeout=60
        )
        versions = []
        for line in result.stdout.splitlines():
            if line.strip().startswith('v') and 'Available versions' not in line and '---' not in line:
                version = line.split()[0]
                if all(c in 'v0123456789.' for c in version):
                    if len(version) <= 4:
                        continue
                    versions.append(version)
        versions.sort(key=Version, reverse=True)
        # versions.insert(0, "nightly")
        return versions

    except Exception as e:
        print(f"è·å– TiDB ç‰ˆæœ¬å¤±è´¥: {e}")
        return ["v8.1.0", "v8.0.0", "v7.5.1", "v7.1.3", "v6.5.9", "v6.1.7", "v5.4.3", "v4.0.16"]


def run_sql_on_tidb(sql, port):
    """åœ¨æŒ‡å®šçš„ TiDB å®ä¾‹ä¸Šæ‰§è¡Œ SQL"""
    result_str = ""
    try:
        conn = mysql.connector.connect(
            host='127.0.0.1',
            port=port,
            user='root',
            password='',
            database='test',
            autocommit=True,
            connection_timeout=20
        )
        cursor = conn.cursor()
        for stmt in sql.split(';'):
            if stmt.strip():
                cursor.execute(stmt)
                if cursor.with_rows:
                    rows = cursor.fetchall()
                    result_str += str(rows) + "\n"
        conn.commit()
        cursor.close()
        conn.close()
        return result_str, True
    except mysql.connector.Error as err:
        print(f"SQL æ‰§è¡Œå¤±è´¥")
        return str(err), False


def run_other_check(script_content, port, task_id):
    """æ‰§è¡Œå…¶ä»–æ£€æŸ¥è„šæœ¬"""
    tasks[task_id]['log'].append("--- å¼€å§‹å…¶ä»–æ£€æŸ¥ ---")

    # 1. è·å– TiDB æ—¥å¿—ç›®å½•
    log_dir_query = "show config where type='tidb' and name='log.file.filename';"
    try:
        result, success = run_sql_on_tidb(log_dir_query, port)
        if not success or not result:
            msg = "è·å– TiDB æ—¥å¿—ç›®å½•å¤±è´¥ã€‚"
            tasks[task_id]['log'].append(f"âŒ {msg}")
            return "Failure", msg

        try:
            data_list = ast.literal_eval(result)
            log_file_path = data_list[0][3]
        except (ValueError, SyntaxError) as e:
            print(f"è§£æå­—ç¬¦ä¸²æ—¶å‡ºé”™: {e}")

        tasks[task_id]['log'].append(f"âœ… æˆåŠŸè·å–åˆ°tidbæ—¥å¿—ç›®å½•: {log_file_path}")
        # e.g., /Users/lt/.tiup/data/Ux1ux8z/tidb-0/tidb.log -> /Users/lt/.tiup/data/Ux1ux8z/
        base_dir = os.path.dirname(os.path.dirname(log_file_path))
        tasks[task_id]['log'].append(f"âœ… è„šæœ¬å°†ä¼šåœ¨æ­¤åŸºç¡€ç›®å½•æ‰§è¡Œ: {base_dir}")

    except Exception as e:
        msg = f"è§£æ TiDB æ—¥å¿—ç›®å½•æ—¶å‡ºé”™: {e}"
        tasks[task_id]['log'].append(f"âŒ {msg}")
        return "Failure", msg

    # 2. ä¿å­˜å¹¶æ‰§è¡Œè„šæœ¬
    script_path = os.path.join(base_dir, f"check_script_{task_id[:8]}.sh")
    try:
        with open(script_path, 'w') as f:
            f.write("#!/bin/bash\n")
            f.write(script_content)

        # èµ‹äºˆè„šæœ¬æ‰§è¡Œæƒé™
        st = os.stat(script_path)
        os.chmod(script_path, st.st_mode | stat.S_IEXEC)
        tasks[task_id]['log'].append(f"âœ… æ£€æŸ¥è„šæœ¬å·²ä¿å­˜åˆ°: {script_path}")

        # æ‰§è¡Œè„šæœ¬
        tasks[task_id]['log'].append(f"ğŸš€ æ‰§è¡Œæ£€æŸ¥è„šæœ¬...")
        process = subprocess.run(
            ['/bin/bash', script_path],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=base_dir
        )

        script_output = process.stdout.strip() + "\n" + process.stderr.strip()
        tasks[task_id]['log'].append(f"è„šæœ¬è¾“å‡º:\n{script_output}")
        print("return code:",process.returncode)

        # 3. æ ¹æ®è¿”å›å€¼åˆ¤æ–­ç»“æœ
        if process.returncode == 0:
            tasks[task_id]['log'].append("âœ… å…¶ä»–æ£€æŸ¥é€šè¿‡ (è„šæœ¬è¿”å›å€¼ä¸º 0)ã€‚")
            return "Success", script_output
        else:
            tasks[task_id]['log'].append(f"âŒ å…¶ä»–æ£€æŸ¥å¤±è´¥ (è„šæœ¬è¿”å›å€¼ä¸º {process.returncode})ã€‚")
            return "Failure", script_output

    except Exception as e:
        msg = f"æ‰§è¡Œæ£€æŸ¥è„šæœ¬æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}"
        tasks[task_id]['log'].append(f"âŒ {msg}")
        return "Failure", msg
    finally:
        # æ¸…ç†è„šæœ¬æ–‡ä»¶
        if os.path.exists(script_path):
            os.remove(script_path)


def test_single_version(version, sql, expected_sql_result, other_check_script, task_id, index, cleanup_after=False,
                        commit=''):
    """ä½¿ç”¨ tiup playground å¯åŠ¨ä¸€ä¸ª TiDB é›†ç¾¤å¹¶æ‰§è¡Œæµ‹è¯•"""
    port_offset = random.randint(10000, 30000)
    sql_port = 4000 + port_offset
    dashboard_port = 2379 + port_offset

    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    log_filename = f"{log_dir}/task_{task_id[:8]}_{version}.log"
    tasks[task_id]['log'].append(f"task_id: {task_id[:8]}")

    if commit != '':
        log_message = f"commit {commit}: å‡†å¤‡å¯åŠ¨é›†ç¾¤ (ç«¯å£åç§»: {port_offset}, SQL Port: {sql_port})..."
    else:
        log_message = f"ç‰ˆæœ¬ {version}: å‡†å¤‡å¯åŠ¨é›†ç¾¤ (ç«¯å£åç§»: {port_offset}, SQL Port: {sql_port})..."

    tasks[task_id]['log'].append(log_message)

    process = None
    log_file = None
    tidb_number = COMPONENT_COUNTS['tidb']
    tikv_number = COMPONENT_COUNTS['tikv']
    pd_number = COMPONENT_COUNTS['pd']
    tiflash_number = COMPONENT_COUNTS['tiflash']

    if commit:
        version_commit = f"{version}-{commit}"
        result_data = {'version': version_commit}
    else:
        result_data = {'version': version}

    try:
        log_file = open(log_filename, 'w', encoding='utf-8')
        if commit != '':
            binary_full_path = os.path.join(TIDB_REPO_PATH, TIDB_BINARY_PATH)
            cmd = ['tiup', 'playground', f'--db.binpath={binary_full_path}', version, f'--port-offset={port_offset}',
                   '--without-monitor', '--kv', f'{tikv_number}', '--tiflash', f'{tiflash_number}',
                   '--pd', f'{pd_number}', '--db', f'{tidb_number}']
            print("install binary cmd:", cmd)
        else:
            cmd = ['tiup', 'playground', version, f'--port-offset={port_offset}', '--without-monitor',
                   '--kv', f'{tikv_number}', '--tiflash', f'{tiflash_number}',
                   '--pd', f'{pd_number}', '--db', f'{tidb_number}']
            print("install version cmd:", cmd)

        # ä½¿ç”¨ Popen å¯åŠ¨éé˜»å¡çš„å­è¿›ç¨‹
        process = subprocess.Popen(cmd, stdout=log_file, stderr=log_file, text=True, encoding='utf-8')

        # å°†è¿›ç¨‹å¯¹è±¡å’Œç‰ˆæœ¬ä¿¡æ¯å­˜å…¥ taskï¼Œä»¥ä¾¿åç»­æ¸…ç†
        tasks[task_id]['processes'].append({'version': version, 'process': process, 'offset': port_offset, 'log_file': log_filename})

        if commit != '':
            log_message = f"commit {commit}: é›†ç¾¤è¿›ç¨‹å·²å¯åŠ¨ (PID: {process.pid})ï¼Œç­‰å¾… TiDB æœåŠ¡å°±ç»ª..."
        else:
            log_message = f"ç‰ˆæœ¬ {version}: é›†ç¾¤è¿›ç¨‹å·²å¯åŠ¨ (PID: {process.pid})ï¼Œç­‰å¾… TiDB æœåŠ¡å°±ç»ª..."
        tasks[task_id]['log'].append(log_message)

        ready = False
        # ç­‰å¾… TiDB å‡†å¤‡å°±ç»ªï¼Œæœ€å¤šç­‰å¾… 180 ç§’
        for _ in range(36):
            time.sleep(5)
            try:
                conn = mysql.connector.connect(host='127.0.0.1', port=sql_port, user='root', password='',
                                               connection_timeout=5)
                conn.close()
                ready = True
                if commit != '':
                    log_message = f"commit {commit}: TiDB æœåŠ¡åœ¨ç«¯å£ {sql_port} ä¸Šå·²å°±ç»ªã€‚"
                else:
                    log_message = f"ç‰ˆæœ¬ {version}: TiDB æœåŠ¡åœ¨ç«¯å£ {sql_port} ä¸Šå·²å°±ç»ªã€‚"
                tasks[task_id]['log'].append(log_message)
                break
            except mysql.connector.Error:
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦æ„å¤–é€€å‡º
                if process.poll() is not None:
                    raise Exception(f"TiUP è¿›ç¨‹æ„å¤–é€€å‡ºã€‚Stderr: {process.stderr.read()}")
                continue

        if not ready:
            raise Exception("TiDB æœåŠ¡å¯åŠ¨è¶…æ—¶")

        # å¦‚æœä¸Šcommit å®šä½è¿‡ç¨‹ï¼Œéœ€è¦åœ¨æµ‹è¯•å‰ç¡®è®¤ä¸€ä¸‹commitæ˜¯å¦ä¸€è‡´ã€‚
        if commit != '':
            sql_check = 'select tidb_version();'
            v_result, success = run_sql_on_tidb(sql_check, sql_port)

            if not success or commit not in ''.join(v_result.split()):
                raise Exception(f"TiDB binary ç‰ˆæœ¬ä¸æ­£ç¡®! æœŸæœ›åŒ…å« {commit}, å®é™…ä¸º {v_result}")
            tasks[task_id]['log'].append("âœ… TiDB binary ç‰ˆæœ¬æ£€æŸ¥é€šè¿‡ã€‚")

        # --- æ‰§è¡Œæ£€æŸ¥ ---
        sql_check_passed = None
        other_check_passed = None

        # 1. SQL ç»“æœæ£€æŸ¥
        if expected_sql_result is not None:
            actual_sql_resultstr, success = run_sql_on_tidb(sql, sql_port)
            print("actual result:", actual_sql_resultstr)
            result_data.update({'expected_sql': expected_sql_result, 'actual_sql': actual_sql_resultstr})

            if expected_sql_result.strip():
                if ''.join(expected_sql_result.split()) in ''.join(actual_sql_resultstr.split()):
                    sql_check_passed = True
                else:
                    sql_check_passed = False
            else:
                # if expected is emptyï¼Œthen sql executed success means check pass.
                if success:
                    print("expected sql is empty, and sql executed success")
                    sql_check_passed = True
                else: sql_check_passed = False
        # 2. å…¶ä»–æ£€æŸ¥
        if other_check_script.strip():
            other_status, other_output = run_other_check(other_check_script, sql_port, task_id)
            result_data.update({'other_check_status': other_status, 'other_check_output': other_output})
            other_check_passed = (other_status == "Success")
        # 3. ç»¼åˆåˆ¤æ–­æœ€ç»ˆç»“æœ
        if sql_check_passed is None and other_check_passed is None:
            # This case is pre-validated in start_locate, but as a safeguard
            raise Exception("æ²¡æœ‰æä¾›ä»»ä½•æ£€æŸ¥æ ‡å‡†ã€‚")

        final_status = "Success"  # Assume success
        if sql_check_passed is False or other_check_passed is False:
            final_status = "Failure"

        result_data.update({
                'status': final_status,
                'sql_port': sql_port,
                'dashboard_port': dashboard_port
        })
    except Exception as e:
        error_msg = f"æµ‹è¯•ç‰ˆæœ¬ {version} æ—¶å‘ç”Ÿé”™è¯¯: {e}"
        tasks[task_id]['log'].append(error_msg)
        result_data = {'version': version, 'status': 'Failure', 'error': str(e)}
    finally:
        # åœ¨äºŒåˆ†æŸ¥æ‰¾æ¨¡å¼ä¸‹ï¼Œæµ‹è¯•å®Œä¸€ä¸ªç‰ˆæœ¬å°±ç«‹å³æ¸…ç†
        if log_file:
            log_file.close()
        if cleanup_after and process:
            if commit != '':
                tasks[task_id]['log'].append(f"commit {commit}: æµ‹è¯•å®Œæˆï¼Œæ¸…ç†é›†ç¾¤ (PID: {process.pid})...")
            else:
                tasks[task_id]['log'].append(f"ç‰ˆæœ¬ {commit}: æµ‹è¯•å®Œæˆï¼Œæ¸…ç†é›†ç¾¤ (PID: {process.pid})...")
            process.terminate()
            process.wait()

    tasks[task_id]['results'][index] = result_data


# --- è·¯ç”± ---
@app.route('/locales/<path:filename>')
def serve_locales(filename):
    """This route serves static files from the 'locales' directory."""
    # Now that it's imported, this function call will work correctly.
    return send_from_directory(os.path.join(app.root_path, 'locales'), filename)


@app.route('/')
def index():
    versions = get_tidb_versions()
    return render_template('index.html', versions=versions)


@app.route('/locate')
def locate_page():
    return render_template('locate.html')


@app.route('/start_test', methods=['POST'])
def start_test():
    global COMPONENT_COUNTS

    data = request.json
    selected_versions = data.get('versions', [])
    sql = data.get('sql')
    print("testcase:", sql)
    expected_sql = data.get('expected_sql_result', '').strip()
    other_script = data.get('other_check_script', '').strip()

    if not selected_versions:
        return jsonify({'error': 'è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰ˆæœ¬ã€‚'}), 400

    tidb_count = int(data.get('tidb') or COMPONENT_COUNTS['tidb'])
    tikv_count = int(data.get('tikv') or COMPONENT_COUNTS['tikv'])
    pd_count = int(data.get('pd') or COMPONENT_COUNTS['pd'])
    tiflash_count = int(data.get('tiflash') or COMPONENT_COUNTS['tiflash'])

    COMPONENT_COUNTS = {
        'tidb': tidb_count,
        'tikv': tikv_count,
        'pd': pd_count,
        'tiflash': tiflash_count
    }
    print(f"[*] Global component counts have been updated to: {COMPONENT_COUNTS}")

    print("æ”¶åˆ°å®šä½ä»»åŠ¡è¯·æ±‚:")
    print(f"  - Bug ç‰ˆæœ¬: {selected_versions}")
    print(f"  - SQL: {sql}")
    print(f"  - é¢„æœŸç»“æœ: {expected_sql}")

    task_id = str(uuid4())
    tasks[task_id] = {
        'status': 'running', 'log': [],
        'results': [{} for _ in selected_versions],  # å ä½
        'processes': [], 'type': 'test'
    }
    session.setdefault('task_ids', []).append(task_id)
    session.modified = True

    threads = []
    for i, version in enumerate(selected_versions):
        thread = threading.Thread(target=test_single_version, args=(version, sql, expected_sql, other_script, task_id, i, False, ''))
        threads.append(thread)
        thread.start()

    def wait_for_completion():
        for t in threads:
            t.join()
        tasks[task_id]['status'] = 'complete'

    threading.Thread(target=wait_for_completion).start()

    return jsonify({'task_id': task_id})


def run_binary_search_with_version(start_v_str, end_v_str, sql, expected_sql, other_check, task_id):
    """äºŒåˆ†æŸ¥æ‰¾é€»è¾‘"""
    all_versions = get_tidb_versions()

    def commit_binary_search_logic(start_version, end_version):
        commits = get_commit_list(start_version, end_version, task_id)
        if not commits:
            print("åœ¨æŒ‡å®šçš„ tag èŒƒå›´å†…æœªæ‰¾åˆ°ä»»ä½• commitã€‚")
            return

        tasks[task_id]['log'].append(f"å¼€å§‹åœ¨ {commits[0]} åˆ° {commits[-1]} ä¹‹é—´è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾...")

        low, high = 0, len(commits) - 1
        first_bad_commit = None

        while low <= high:
            mid = (low + high) // 2
            commit_sha = commits[mid]

            tasks[task_id]['log'].append(f"\n--- æ­£åœ¨æµ‹è¯•ç¬¬ {mid + 1}/{len(commits)} ä¸ª commit: {commit_sha[:12]} ---")
            binary_path = compile_at_commit(commit_sha, task_id, end_version)
            if binary_path is None:
                print(f"ğŸ‘ [BAD] Commit {commit_sha[:12]} ç¼–è¯‘å¤±è´¥ã€‚")
                # first_bad_commit = commit_sha
                high = mid - 1
                continue
            result_index = len(tasks[task_id]['results'])
            tasks[task_id]['results'].append({})
            test_single_version(end_version, sql, expected_sql, other_check, task_id, result_index, cleanup_after=True,
                                commit=commit_sha)

            result_data = tasks[task_id]['results'][result_index]

            if result_data.get('status') == 'Failure':
                first_bad_commit = commit_sha
                high = mid - 1
            elif result_data.get('status') == 'Success':
                low = mid + 1
            else:
                tasks[task_id]['log'].append(f"ç‰ˆæœ¬ {commit_sha} æµ‹è¯•æ—¶å‘ç”Ÿç¯å¢ƒé”™è¯¯ï¼Œå®šä½ä¸­æ­¢ã€‚")
                tasks[task_id]['status'] = 'error'
                return None

        return first_bad_commit

    def binary_search_logic(start_version, end_version):
        search_space = [
            v for v in all_versions
            if Version(v) >= Version(start_version) and Version(v) <= Version(end_version)
        ]
        search_space.sort(key=Version)

        tasks[task_id]['log'].append(f"å¼€å§‹åœ¨ {start_version} åˆ° {end_version} ä¹‹é—´è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾...")
        tasks[task_id]['log'].append(f"å¾…æŸ¥æ‰¾çš„ç‰ˆæœ¬åˆ—è¡¨: {search_space}")

        low, high = 0, len(search_space) - 1
        first_bad_version = None

        while low <= high:
            mid_idx = (low + high) // 2
            version_to_test = search_space[mid_idx]

            result_index = len(tasks[task_id]['results'])
            tasks[task_id]['results'].append({})
            test_single_version(version_to_test, sql, expected_sql, other_check, task_id, result_index,
                                cleanup_after=True)

            result_data = tasks[task_id]['results'][result_index]

            if result_data.get('status') == 'Failure':
                first_bad_version = version_to_test
                high = mid_idx - 1
            elif result_data.get('status') == 'Success':
                low = mid_idx + 1
            else:
                tasks[task_id]['log'].append(f"ç‰ˆæœ¬ {version_to_test} æµ‹è¯•æ—¶å‘ç”Ÿç¯å¢ƒé”™è¯¯ï¼Œå®šä½ä¸­æ­¢ã€‚")
                tasks[task_id]['status'] = 'error'
                return None

        return first_bad_version

    # 1. å¯¹èµ·å§‹ç‰ˆæœ¬è¿›è¡ŒåŸºçº¿æ£€æŸ¥
    tasks[task_id]['log'].append(f"\n--- æ­£åœ¨æ‰§è¡ŒåŸºçº¿æ£€æŸ¥ (èµ·å§‹ç‚¹): {start_v_str} ---")
    start_index = len(tasks[task_id]['results'])
    tasks[task_id]['results'].append({})
    test_single_version(start_v_str, sql, expected_sql, other_check, task_id, start_index, cleanup_after=True)

    start_result = tasks[task_id]['results'][start_index]
    if start_result.get('status') == 'Failure':
        error_msg = "æœ¬èŒƒå›´å†…æ— æ³•æ‰¾åˆ°å¼•å…¥é—®é¢˜çš„ pr,è¯·åœ¨æ›´æ—©çš„ç‰ˆæœ¬æˆ–è€… commit èŒƒå›´å†…æŸ¥æ‰¾"
        tasks[task_id]['log'].append(f"\nâŒ åŸºçº¿æ£€æŸ¥å¤±è´¥: èµ·å§‹ç‰ˆæœ¬ {start_v_str} å·²ä¸ç¬¦åˆé¢„æœŸã€‚")
        tasks[task_id]['final_result'] = error_msg
        tasks[task_id]['status'] = 'complete'
        return

    # 2. å¯¹ç»“æŸç‰ˆæœ¬ï¼ˆBugä¸ŠæŠ¥ç‰ˆæœ¬ï¼‰è¿›è¡Œå¥å…¨æ€§æ£€æŸ¥
    tasks[task_id]['log'].append(f"\n--- æ­£åœ¨æ‰§è¡Œå¥å…¨æ€§æ£€æŸ¥ (ç»“æŸç‚¹): {end_v_str} ---")
    end_index = len(tasks[task_id]['results'])
    tasks[task_id]['results'].append({})
    test_single_version(end_v_str, sql, expected_sql, other_check, task_id, end_index, cleanup_after=True)

    end_result = tasks[task_id]['results'][end_index]
    if end_result.get('status') == 'Success':
        error_msg = f"å¥å…¨æ€§æ£€æŸ¥å¤±è´¥: 'Bug ä¸ŠæŠ¥ç‰ˆæœ¬' ({end_v_str}) çš„æµ‹è¯•ç»“æœä¸ºæˆåŠŸï¼Œæ— æ³•è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾ã€‚"
        tasks[task_id]['log'].append(f"\nâŒ {error_msg}")
        tasks[task_id]['final_result'] = error_msg
        tasks[task_id]['status'] = 'complete'
        return

    if start_v_str == "v5.4.0":
        tasks[task_id]['log'].append("æ£€æŸ¥åŸºçº¿ç‰ˆæœ¬ v5.4.0...")
        # ã€ä¿®å¤ã€‘ä¸º v5.4.0 æµ‹è¯•æ·»åŠ å ä½ç¬¦
        tasks[task_id]['results'].append({})
        test_single_version("v5.4.0", sql, expected_sql, other_check, task_id, 0)
        # ç¡®ä¿æµ‹è¯•çº¿ç¨‹æœ‰æ—¶é—´å†™å…¥ç»“æœ
        time.sleep(0.1)
        v540_result = tasks[task_id]['results'][0]

        if v540_result.get('status') == 'Failure' and 'error' not in v540_result:
            tasks[task_id]['log'].append("v5.4.0 ä¸Šçš„ç»“æœå·²ä¸ç¬¦åˆé¢„æœŸï¼Œå°†åœ¨ v4.0.0 å’Œ v5.3.0 ä¹‹é—´æŸ¥æ‰¾ã€‚")
            start_v_str = "v4.0.0"
            end_v_str = "v5.3.0"
        elif v540_result.get('status') == 'Success':
            start_v_str = "v5.4.1"

    found_version = binary_search_logic(start_v_str, end_v_str)
    tasks[task_id]['log'].append(f"\n----å®šä½åˆ°ç¬¬ä¸€ä¸ªå‡ºé”™çš„ç‰ˆæœ¬æ˜¯: {found_version}----")
    tasks[task_id][
        'final_result'] = f"å®šä½åˆ°ç¬¬ä¸€ä¸ªå‡ºé”™çš„ç‰ˆæœ¬æ˜¯: {found_version}" if found_version else f"åœ¨ {start_v_str}-{end_v_str} èŒƒå›´å†…æœªæ‰¾åˆ°ä¸ç¬¦åˆé¢„æœŸçš„ç‰ˆæœ¬ã€‚"

    if found_version:
        tidb_versions = get_tidb_versions()
        start_version_index = tidb_versions.index(found_version) + 1
        found_commit = commit_binary_search_logic(tidb_versions[start_version_index], found_version)
        tasks[task_id][
            'final_result'] = f"å®šä½åˆ°ç¬¬ä¸€ä¸ªå‡ºé”™çš„commitæ˜¯: {found_version}-{found_commit}, " if found_commit else f"åœ¨ {start_v_str} èŒƒå›´å†…æœªæ‰¾åˆ°ä¸ç¬¦åˆé¢„æœŸçš„commitã€‚"
        if found_commit:
            try:
                output = run_command(["git", "show", found_commit, "--no-patch", ], work_dir=TIDB_REPO_PATH)
                # tasks[task_id]['log'].append(f"âœ… import issue and pr: {output}")
                tasks[task_id][
                    'final_result'] = f"å®šä½åˆ°ç¬¬ä¸€ä¸ªå‡ºé”™çš„commitæ˜¯: {found_version}-{found_commit}\n\nimport issue and pr: {output}, " if found_commit else f"åœ¨ {start_v_str} èŒƒå›´å†…æœªæ‰¾åˆ°ä¸ç¬¦åˆé¢„æœŸçš„commitã€‚"

            except RuntimeError as e:
                print(e)

    tasks[task_id]['status'] = 'complete'


def run_binary_search_with_commit(start_commit, end_commit, branch, sql, expected_sql, other_check, task_id):
    """äºŒåˆ†æŸ¥æ‰¾é€»è¾‘"""

    def commit_binary_search_logic(start_commit, end_commit, branch):
        try:
            print(f"[*] æ­£åœ¨åˆ‡æ¢åˆ°åˆ†æ”¯: {branch}")
            subprocess.run(["git", "checkout", "-f", branch], cwd=TIDB_REPO_PATH, check=True, capture_output=True,
                           text=True)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"åˆ‡æ¢åˆ°åˆ†æ”¯ '{branch}' å¤±è´¥: {e.stderr.strip()}")
            return
        command = ["git", "rev-list", "--reverse", f"{start_commit}..{end_commit}"]
        try:
            result = subprocess.run(
                command,
                cwd=TIDB_REPO_PATH,
                check=True,
                capture_output=True,
                text=True
            )

            # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²è¾“å‡ºï¼Œå¹¶è¿‡æ»¤æ‰å¯èƒ½çš„ç©ºè¡Œ
            commits_after_start = [line for line in result.stdout.strip().split('\n') if line]

            # å°†èµ·å§‹ commit æ·»åŠ åˆ°åˆ—è¡¨çš„å¼€å¤´ï¼Œæ„æˆå®Œæ•´çš„åŒ…å«èŒƒå›´
            commits = [start_commit] + commits_after_start

        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"æ‰§è¡Œ 'git rev-list' å¤±è´¥: {e.stderr.strip()}")

        if not commits:
            print("åœ¨æŒ‡å®šçš„ tag èŒƒå›´å†…æœªæ‰¾åˆ°ä»»ä½• commitã€‚")
            return None

        tasks[task_id]['log'].append(f"å¼€å§‹åœ¨ {commits[0]} åˆ° {commits[-1]} ä¹‹é—´è¿›è¡ŒäºŒåˆ†æŸ¥æ‰¾...")

        low, high = 0, len(commits) - 1
        first_bad_commit = None

        while low <= high:
            mid = (low + high) // 2
            commit_sha = commits[mid]

            tasks[task_id]['log'].append(f"\n--- æ­£åœ¨æµ‹è¯•ç¬¬ {mid + 1}/{len(commits)} ä¸ª commit: {commit_sha[:12]} ---")
            i_v = branch
            if str(branch).find('release-') != -1:
                i_v = str(branch).lstrip('release-') + '.0'

            binary_path = compile_at_commit(commit_sha, task_id, i_v)
            if binary_path is None:
                print(f"ğŸ‘ [BAD] Commit {commit_sha[:12]} ç¼–è¯‘å¤±è´¥ã€‚")
                # first_bad_commit = commit_sha
                high = mid - 1
                continue
            result_index = len(tasks[task_id]['results'])
            tasks[task_id]['results'].append({})  # å ä½
            # cleanup_after=True è¡¨ç¤ºæµ‹è¯•å®Œå°±æ¸…ç†
            if branch == 'master':
                install_version = 'nightly'
            elif str(branch).find('release-') != -1:
                install_version = 'v' + str(branch).lstrip('release-') + '.0'
            else:
                tasks[task_id]['log'].append(f"branch format is incorrect")
                return
            test_single_version(install_version, sql, expected_sql, other_check, task_id, result_index,
                                cleanup_after=True, commit=commit_sha)

            result_data = tasks[task_id]['results'][result_index]

            if result_data.get('status') == 'Failure':
                first_bad_commit = commit_sha
                high = mid - 1
            elif result_data.get('status') == 'Success':
                low = mid + 1
            else:
                tasks[task_id]['log'].append(f"ç‰ˆæœ¬ {commit_sha} æµ‹è¯•æ—¶å‘ç”Ÿç¯å¢ƒé”™è¯¯ï¼Œå®šä½ä¸­æ­¢ã€‚")
                tasks[task_id]['status'] = 'error'
                return None

        return first_bad_commit

    # åŸºçº¿ç‰ˆæœ¬æµ‹è¯•
    start_commit_to_test = start_commit
    tasks[task_id]['log'].append(f"\n--- æ­£åœ¨æ‰§è¡ŒåŸºçº¿æ£€æŸ¥ (èµ·å§‹ Commit): {start_commit_to_test[:7]} ---")

    def test_a_commit(commit_sha, index):
        install_version = 'nightly' if branch == 'master' else f'v{branch.replace("release-", "")}.0'
        binary_path = compile_at_commit(commit_sha, task_id, install_version)
        if binary_path is None:
            tasks[task_id]['results'][index] = {'version': commit_sha, 'status': 'Failure', 'error': 'ç¼–è¯‘å¤±è´¥'}
            return
        test_single_version(install_version, sql, expected_sql, other_check, task_id, index, cleanup_after=True,
                            commit=commit_sha)

    start_index = len(tasks[task_id]['results'])
    tasks[task_id]['results'].append({})
    test_a_commit(start_commit_to_test, start_index)

    start_result = tasks[task_id]['results'][start_index]
    if start_result.get('status') == 'Failure':
        error_msg = "æœ¬èŒƒå›´å†…æ— æ³•æ‰¾åˆ°å¼•å…¥é—®é¢˜çš„pr,è¯·åœ¨æ›´æ—©çš„ç‰ˆæœ¬æˆ–è€…commit èŒƒå›´å†…æŸ¥æ‰¾"
        tasks[task_id]['log'].append(f"\nâŒ åŸºçº¿æ£€æŸ¥å¤±è´¥: èµ·å§‹ Commit {start_commit_to_test[:7]} å·²ä¸ç¬¦åˆé¢„æœŸã€‚")
        tasks[task_id]['final_result'] = error_msg
        tasks[task_id]['status'] = 'complete'
        return

    # å¼€å§‹äºŒåˆ†æµ‹è¯•
    found_commit = commit_binary_search_logic(start_commit, end_commit, branch)
    output = ""
    if found_commit:
        try:
            output = run_command(["git", "show", found_commit, "--no-patch"], work_dir=TIDB_REPO_PATH)
        except Exception as e:
            print(e)
    tasks[task_id][
        'final_result'] = f"å®šä½åˆ°ç¬¬ä¸€ä¸ªå‡ºé”™çš„commitæ˜¯: {found_commit}\n\nimport issue and pr: {output}, " if found_commit else f"åœ¨ {branch} èŒƒå›´å†…æœªæ‰¾åˆ°ä¸ç¬¦åˆé¢„æœŸçš„commitã€‚"

    tasks[task_id]['status'] = 'complete'


@app.route('/start_locate', methods=['POST'])
def start_locate():
    global COMPONENT_COUNTS

    data = request.json
    if not data:
        return jsonify({'error': 'Invalid JSON payload'}), 400

    locate_mode = data.get('locate_mode')
    sql = data.get('sql')
    expected_sql_result = data.get('expected_sql_result', '').strip()
    other_check_script = data.get('other_check_script', '').strip()

    print("æ”¶åˆ°çš„é¢„æœŸ SQL ç»“æœ:", expected_sql_result)
    print("æ”¶åˆ°çš„å…¶ä»–æ£€æŸ¥è„šæœ¬:", other_check_script)

    tidb_count = int(data.get('tidb') or COMPONENT_COUNTS['tidb'])
    tikv_count = int(data.get('tikv') or COMPONENT_COUNTS['tikv'])
    pd_count = int(data.get('pd') or COMPONENT_COUNTS['pd'])
    tiflash_count = int(data.get('tiflash') or COMPONENT_COUNTS['tiflash'])

    COMPONENT_COUNTS = {
        'tidb': tidb_count,
        'tikv': tikv_count,
        'pd': pd_count,
        'tiflash': tiflash_count
    }
    print(data)
    # print(f"[*] Global component counts have been updated to: {COMPONENT_COUNTS}")

    bug_version = start_version_str = branch = start_commit = end_commit = ''

    if locate_mode == 'version':
        bug_version = data.get('bug_version')
        start_version_str = data.get('start_version') or "v5.4.0"
        if not bug_version:
            return jsonify({'error': 'bug_version is required for version mode'}), 400
        try:
            if Version(start_version_str) >= Version(bug_version):
                return jsonify({'error': 'â€œèµ·å§‹ç‰ˆæœ¬â€ä¸èƒ½ç­‰äºæˆ–è€…æ™šäºâ€œbug ä¸ŠæŠ¥ç‰ˆæœ¬â€'}), 400
        except Exception:
            return jsonify({'error': 'ç‰ˆæœ¬å·æ ¼å¼æ— æ•ˆ'}), 400
        print("æ”¶åˆ°å®šä½ä»»åŠ¡è¯·æ±‚:")
        print(f"  - locate mode: {locate_mode}")
        print(f"  - start version: {start_version_str}")
        print(f"  - end version: {bug_version}")
        print(f"  - SQL: {sql}")
        print(f"  - é¢„æœŸç»“æœ: {expected_sql_result}")
    elif locate_mode == 'commit':
        branch = data.get('branch')
        if (branch != 'master') and (str(branch).find('release-') == -1):
            return jsonify({'error': 'branch format is incorrect, should be release-x.x'}), 400
        start_commit = data.get('start_commit')
        end_commit = data.get('end_commit')
        if not all([branch, start_commit, end_commit]):
            return jsonify({'error': 'branch, start_commit, and end_commit are required for commit mode'}), 400
        print("æ”¶åˆ°å®šä½ä»»åŠ¡è¯·æ±‚:")
        print(f"  - locate mode: {locate_mode}")
        print(f"  - start commit: {start_commit}")
        print(f"  - end commit: {end_commit}")
        print(f"  - branch: {branch}")
        print(f"  - SQL: {sql}")
        print(f"  - é¢„æœŸç»“æœ: {expected_sql_result}")
    else:
        return jsonify({'error': f'Unknown locate_mode: {locate_mode}'}), 400

    task_id = str(uuid4())
    tasks[task_id] = {
        'status': 'running', 'log': [], 'results': [],
        'processes': [], 'type': 'locate'
    }
    session.setdefault('task_ids', []).append(task_id)
    session.modified = True

    if locate_mode == 'version':
        thread = threading.Thread(target=run_binary_search_with_version,
                              args=(start_version_str, bug_version, sql, expected_sql_result, other_check_script, task_id))
    elif locate_mode == 'commit':
        thread = threading.Thread(target=run_binary_search_with_commit,
                                  args=(start_commit, end_commit, branch, sql, expected_sql_result, other_check_script, task_id))
    thread.start()

    return jsonify({'task_id': task_id})


@app.route('/status/<task_id>')
def task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€ (ä¿®æ­£ç‰ˆ)"""
    task = tasks.get(task_id)
    if not task:
        return jsonify({'status': 'not_found'}), 404

    # åˆ›å»ºä¸€ä¸ªå¯åºåˆ—åŒ–ï¼ˆå¯è½¬æ¢ä¸º JSONï¼‰çš„ä»»åŠ¡å‰¯æœ¬
    # ä¸è¦ç›´æ¥ä¿®æ”¹åŸå§‹çš„ task å­—å…¸ï¼Œå› ä¸ºæˆ‘ä»¬è¿˜éœ€è¦é‡Œé¢çš„ Popen å¯¹è±¡æ¥æ¸…ç†è¿›ç¨‹
    serializable_task = {
        'status': task.get('status'),
        'log': task.get('log', []),
        'results': task.get('results', []),
        'type': task.get('type'),
        'final_result': task.get('final_result'),
        'processes_info': []
    }

    # éå†åŸå§‹ä»»åŠ¡ä¸­çš„è¿›ç¨‹åˆ—è¡¨
    if 'processes' in task:
        for proc_info in task['processes']:
            process = proc_info.get('process')
            # å°† Popen å¯¹è±¡æ›¿æ¢ä¸ºå®ƒçš„ PID (ä¸€ä¸ªç®€å•çš„æ•´æ•°)
            serializable_task['processes_info'].append({
                'version': proc_info.get('version'),
                'pid': process.pid if process else None,
                'offset': proc_info.get('offset'),
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                'is_running': process.poll() is None if process else False
            })

    # è¿”å›è¿™ä¸ªæ¸…ç†è¿‡åçš„ã€å®‰å…¨çš„å­—å…¸
    return jsonify(serializable_task)


@app.route('/clean', methods=['POST'])
def clean_env():
    """æ¸…ç†å½“å‰ session åˆ›å»ºçš„æ‰€æœ‰ tiup playground è¿›ç¨‹å’Œæ—¥å¿—æ–‡ä»¶ (æ›´æ–°ç‰ˆ)"""
    # This line is at the first level of indentation
    task_ids_to_clean = session.get('task_ids', [])
    cleaned_pids = []
    deleted_logs = []
    errors = []

    # The 'for' loop is at the first level
    for task_id in task_ids_to_clean:
        # This block is at the second level of indentation
        task = tasks.get(task_id)
        if not task or not task.get('processes'):
            continue
        
        # This 'for' loop is at the second level
        for proc_info in task['processes']:
            # This block is at the third level of indentation
            # 1. ç»ˆæ­¢è¿›ç¨‹
            process = proc_info.get('process')
            if process and process.poll() is None:
                # This block is at the fourth level
                try:
                    pid = process.pid
                    process.terminate()
                    process.wait(timeout=30)
                    cleaned_pids.append(pid)
                except Exception as e:
                    errors.append(f"æ¸…ç†è¿›ç¨‹ PID {pid} å¤±è´¥: {e}")
            
            # 2. åˆ é™¤æ—¥å¿—æ–‡ä»¶ (This block is at the third level)
            log_file = proc_info.get('log_file')

            if log_file and os.path.exists(log_file):
                try:
                    os.remove(log_file)
                    deleted_logs.append(log_file)
                except OSError as e:
                    errors.append(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶ {log_file} å¤±è´¥: {e}")

    # This block is back at the first level
    session['task_ids'] = []
    session.modified = True
    
    return jsonify({
        'cleaned_pids': cleaned_pids,
        'deleted_logs': deleted_logs,
        'errors': errors
    })


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5001)
